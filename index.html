<!DOCTYPE html>
<html>

  <head>
    <meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta http-equiv="X-UA-Compatible" content="IE=edge">

<title>

  Sobhan  Soleymani


</title>
<meta name="description" content="A simple, whitespace theme for academics. Based on [*folio](https://github.com/bogoli/-folio) design.
">

<!-- Open Graph -->


<!-- Bootstrap & MDB -->
<link href="https://stackpath.bootstrapcdn.com/bootstrap/4.5.2/css/bootstrap.min.css" rel="stylesheet" integrity="sha512-MoRNloxbStBcD8z3M/2BmnT+rg4IsMxPkXaGh2zD6LGNNFE80W3onsAhRcMAMrSoyWL9xD7Ert0men7vR8LUZg==" crossorigin="anonymous">
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/mdbootstrap/4.19.1/css/mdb.min.css" integrity="sha512-RO38pBRxYH3SoOprtPTD86JFOclM51/XTIdEPh5j8sj4tp8jmQIx26twG52UaLi//hQldfrh7e51WzP9wuP32Q==" crossorigin="anonymous" />

<!-- Fonts & Icons -->
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.14.0/css/all.min.css"  integrity="sha512-1PKOgIY59xJ8Co8+NE6FZ+LOAZKjy+KY8iq0G4B3CyeY6wYHN3yt9PW0XpSriVlkMXe40PTKnXrLnZ9+fkDaog==" crossorigin="anonymous">
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/academicons/1.9.0/css/academicons.min.css" integrity="sha512-W4yqoT1+8NLkinBLBZko+dFB2ZbHsYLDdr50VElllRcNt2Q4/GSs6u71UHKxB7S6JEMCp5Ve4xjh3eGQl/HRvg==" crossorigin="anonymous">
<link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons">

<!-- Code Syntax Highlighting -->
<link rel="stylesheet" href="https://gitcdn.link/repo/jwarby/jekyll-pygments-themes/master/github.css" />

<!-- Styles -->

<link rel="icon" href="data:image/svg+xml,<svg xmlns=%22http://www.w3.org/2000/svg%22 viewBox=%220 0 100 100%22><text y=%22.9em%22 font-size=%2290%22>ðŸ”¥</text></svg>">

<link rel="stylesheet" href="/assets/css/main.css">
<link rel="canonical" href="/">

<!-- JQuery -->
<!-- jQuery -->
<script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js" integrity="sha512-bLT0Qm9VnAYZDflyKcBaQ2gg0hSYNQrJ8RilYldYQ1FxQYoCLtUjuuRuZo+fjqhx/qtq/1itJ0C2ejDxltZVFg==" crossorigin="anonymous"></script>


<!-- Theming-->

<script src="/assets/js/theme.js"></script>
<script src="/assets/js/dark_mode.js"></script>






    
<!-- MathJax -->
<script type="text/javascript">
  window.MathJax = {
    tex: {
      tags: 'ams'
    }
  };
</script>
<script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script>
<script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>


  </head>

  <body class="fixed-top-nav ">

    <!-- Header -->

    <header>

    <!-- Nav Bar -->
    <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top">
    <div class="container">
      
      <!-- Navbar Toggle -->
      <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation">
        <span class="sr-only">Toggle navigation</span>
        <span class="icon-bar top-bar"></span>
        <span class="icon-bar middle-bar"></span>
        <span class="icon-bar bottom-bar"></span>
      </button>
      <div class="collapse navbar-collapse text-right" id="navbarNav">
        <ul class="navbar-nav ml-auto flex-nowrap">
          <!-- About -->
          <li class="nav-item active">
            <a class="nav-link" href="/">
              about
              
                <span class="sr-only">(current)</span>
              
            </a>
          </li>
          
          <!-- Other pages -->
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
            <div class = "toggle-container">
              <a id = "light-toggle">
                  <i class="fas fa-moon"></i>
                  <i class="fas fa-sun"></i>
              </a>
            </div>
          
        </ul>
      </div>
    </div>
  </nav>

</header>


    <!-- Content -->

    <div class="container mt-5">
      <div class="post">

  <header class="post-header">


    <!-- Global site tag (gtag.js) - Google Analytics -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-KZ046R10XG"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());

      gtag('config', 'G-KZ046R10XG');
    </script>


    <h1 class="post-title">
     <span class="font-weight-bold">Sobhan</span>  Soleymani
    </h1>
     <p class="desc">Postdoctoral Fellow, <a href="https://lcsee.statler.wvu.edu/">West Virginia University</a>.</p>
  </header>

  <article>
    
    <div class="profile float-right">
      
        <img class="img-fluid z-depth-1 rounded" src="/assets/img/prof_pic.png">
      
      
      <div class="social">
        <div class="contact-icons">
          <a href="mailto:%73%73%6F%6C%65%79%6D%61@%6D%69%78.%77%76%75.%65%64%75"><i class="fas fa-envelope fa-xs" style="font-size: 2rem;"></i></a>

<a href="https://scholar.google.com/citations?user=83AwkccAAAAJ" target="_blank" title="Google Scholar"><i class="ai ai-google-scholar fa-xs" style="font-size: 2rem;"></i></a>


<a href="https://github.com/sbnsl" target="_blank" title="GitHub"><i class="fab fa-github fa-xs" style="font-size: 2rem;"></i></a>
<a href="https://www.linkedin.com/in/sobhan-soleymani" target="_blank" title="LinkedIn"><i class="fab fa-linkedin fa-xs" style="font-size: 2rem;"></i></a>











        </div>
        <div class="contact-note"></div>
      </div>
    

<!--      -->
<!--        <div class="address">-->
<!--          <p>Morgantown, WV</p>
-->
<!--        </div>-->
<!--      -->
    </div>
    

    <div class="clearfix">
      <p>I am a Postdoctoral Fellow in the <a href="https://lcsee.statler.wvu.edu/">Department of Computer Science and Electrical Engineering</a> at West Virginia University, working with <a href="https://nassernasrabadi.faculty.wvu.edu/">Dr. Nasser M. Nasrabadi</a>, where I received my PhD in May 2021. Prior to that, I recieved my MSc from <a href="https://www.epfl.ch/en/">EPFL</a> working with <a href="https://www.epfl.ch/labs/lcav/people/martin-vetterli/">Dr. Martin Vetterli</a> and <a href="https://lu.seas.harvard.edu/">Dr. Yue M. Lu</a>. My broad research intrests are deep learning, machine learning (ML), pattern recognition, and computer vision. Currently, the focus of my research projects and graduate student advising is adversarial ML, network compression, interpretable ML, unsupervised representation learning and applications of deep learning in computer vision and biometrics.</p>

<p><a href="assets/Sobhan_CV.pdf">Curriculum vitae</a></p>

<h2 id="interests">Interests</h2>

<p>I am passionate about any ML and CV topic, but currently, these are the specials:</p>

<ul>
  <li>Multimodal Recognition</li>
  <li>Adversarial ML</li>
  <li>Biometrics</li>
  <li>Face Morphing</li>
  <li>Semi/un/self-supervised Learning</li>
  <li>Network Compression</li>
  <li>Interpretable ML</li>
</ul>


    </div>

    
      <div class="news">
  <h2>news</h2>
  
    <div class="table-responsive">
      <table class="table table-sm table-borderless">
      
      
        <tr>
          <th scope="row">Nov 28, 2021</th>
          <td>
            
              Reviewed three papers for ICLR-2022.

            
          </td>
        </tr>
      
        <tr>
          <th scope="row">Nov 12, 2021</th>
          <td>
            
              One papers accepted at IEEE Transactions on Biometrics, Behavior, and Identity Science (TBIOM).


            
          </td>
        </tr>
      
        <tr>
          <th scope="row">Jul 25, 2021</th>
          <td>
            
              Reviewed three papers for NeurIPS-2021.

            
          </td>
        </tr>
      
        <tr>
          <th scope="row">Mar 25, 2021</th>
          <td>
            
              Two papers accepted at CVPR-2021.

            
          </td>
        </tr>
      
        <tr>
          <th scope="row">May 5, 2020</th>
          <td>
            
              One paper accepted at ECCV-2020.

            
          </td>
        </tr>
      
      </table>
    </div>
  
</div>

    

    
      <div class="publications">
  <h2>selected publications</h2>
  <ol class="bibliography"><li><div class="row">
    <div class="col-sm-2 abbr" style="height:100%;">
        <!--  -->
        <!--    -->
        <!--    <abbr class="badge">TBIOM</abbr>-->
        <!--    -->
        <!--  -->
        
<!--        <img src="assets/img/TBIOM_Fig02.png">-->
         <img class="img-fluid z-depth-1 rounded" src="assets/img/TBIOM_Fig02.png">

        
    </div>

    <div id="14" class="col-sm-8">
        
<!--        <div class="title">Quality-Aware Multimodal Biometric Recognition</div>-->
        <div class="title"> <a href="https://ieeexplore.ieee.org/abstract/document/9631949">Quality-Aware Multimodal Biometric Recognition</a> </div>
        <div class="author">
            
            
            
            
            

            
            
            
            
            Soleymani, Sobhan,
            
            
            
            
            
            
            
            
            

            
            
            
            
            Dabouei, Ali,
            
            
            
            
            
            
            
            
            

            
            
            
            
            Iranmanesh, Fariborz Taherkhani Seyed Mehdi,
            
            
            
            
            
            
            
            
            

            
            
            
            
            Dawson, Jeremy,
            
            
            
            
            
            
            
            
            

            
            
            
            
            and Nasrabadi, Nasser M.
            
            
            
            
            
        </div>

        <div class="periodical">
            
            <em>In IEEE Transactions on Biometrics, Behavior, and Identity Science</em>
            
            
            2021
            
        </div>
        

        <div class="links">
            
            <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
            
            
            
            
            
            
            <a href="https://arxiv.org/pdf/2112.05827.pdf" class="btn btn-sm z-depth-0" role="button" target="_blank">PDF</a>
            
            
            
            
            
            
            
            
        </div>

        <!-- Hidden abstract block -->
        
        <div class="abstract hidden">
            <p>We present a quality-aware multimodal recognition framework that combines representations from multiple biometric traits with varying quality and number of samples to achieve increased recognition accuracy by extracting complimentary identification information based on the quality of the samples. We develop a quality-aware framework for fusing representations of input modalities by weighting their importance using quality scores estimated in a weakly-supervised fashion. This framework utilizes two fusion blocks, each represented by a set of quality-aware and aggregation networks. In addition to architecture modifications, we propose two task-specific loss functions: multimodal separability loss and multimodal compactness loss. The first loss assures that the representations of modalities for a class have comparable magnitudes to provide a better quality estimation, while the multimodal representations of different classes are distributed to achieve maximum discrimination in the embedding space. The second loss, which is considered to regularize the network weights, improves the generalization performance by regularizing the framework. We evaluate the performance by considering three multimodal datasets consisting of face, iris, and fingerprint modalities. The efficacy of the framework is demonstrated through comparison with the state-of-the-art algorithms. In particular, our framework outperforms the rank- and score-level fusion of modalities of BIOMDATA by more than 30% for true acceptance rate at false acceptance rate of 10^-4.</p>
        </div>
        

        <!-- Hidden bibtex block -->
        
    </div>
</div>
</li>
<li><div class="row">
    <div class="col-sm-2 abbr" style="height:100%;">
        <!--  -->
        <!--    -->
        <!--    <abbr class="badge">CVPR</abbr>-->
        <!--    -->
        <!--  -->
        
<!--        <img src="assets/img/supermix.jpg">-->
         <img class="img-fluid z-depth-1 rounded" src="assets/img/supermix.jpg">

        
    </div>

    <div id="13" class="col-sm-8">
        
<!--        <div class="title">SuperMix: Supervising the Mixing Data Augmentation</div>-->
        <div class="title"> <a href="https://openaccess.thecvf.com/content/CVPR2021/html/Dabouei_SuperMix_Supervising_the_Mixing_Data_Augmentation_CVPR_2021_paper.html">SuperMix: Supervising the Mixing Data Augmentation</a> </div>
        <div class="author">
            
            
            
            
            

            
            
            
            
            Dabouei, Ali,
            
            
            
            
            
            
            
            
            

            
            
            
            
            Soleymani, Sobhan,
            
            
            
            
            
            
            
            
            

            
            
            
            
            Taherkhani, Fariborz,
            
            
            
            
            
            
            
            
            

            
            
            
            
            and Nasrabadi, Nasser M.
            
            
            
            
            
        </div>

        <div class="periodical">
            
            <em>In CVPR</em>
            
            
            2021
            
        </div>
        

        <div class="links">
            
            <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
            
            
            
            
            
            
            <a href="https://openaccess.thecvf.com/content/CVPR2021/papers/Dabouei_SuperMix_Supervising_the_Mixing_Data_Augmentation_CVPR_2021_paper.pdf" class="btn btn-sm z-depth-0" role="button" target="_blank">PDF</a>
            
            
            
            
            
            <a href="https://github.com/sbnsl/SuperMix" class="btn btn-sm z-depth-0" role="button" target="_blank">Code</a>
            
            
            
            
        </div>

        <!-- Hidden abstract block -->
        
        <div class="abstract hidden">
            <p>This paper presents a supervised mixing augmentation method termed SuperMix, which exploits the salient regions within input images to construct mixed training samples. SuperMix is designed to obtain mixed images rich in visual features and complying with realistic image priors. To enhance the efficiency of the algorithm, we develop a variant of the Newton iterative method, 65xfaster than gradient descent on this problem. We validate the effectiveness of SuperMix through extensive evaluations and ablation studies on two tasks of object classification and knowledge distillation. On the classification task, SuperMix provides comparable performance to the advanced augmentation methods, such as AutoAugment and RandAugment. In particular, combining SuperMix with RandAugment achieves 78.2% top-1 accuracy on ImageNet with ResNet50. On the distillation task, solely classifying images mixed using the teacherâ€™s knowledge achieves comparable performance to the state-of-the-art distillation methods. Furthermore, on average, incorporating mixed images into the distillation objective improves the performance by 3.4% and 3.1% on CIFAR-100 and ImageNet, respectively.</p>
        </div>
        

        <!-- Hidden bibtex block -->
        
    </div>
</div>
</li>
<li><div class="row">
    <div class="col-sm-2 abbr" style="height:100%;">
        <!--  -->
        <!--    -->
        <!--    <abbr class="badge">CVPR</abbr>-->
        <!--    -->
        <!--  -->
        
<!--        <img src="assets/img/far_cvpr21.jpg">-->
         <img class="img-fluid z-depth-1 rounded" src="assets/img/far_cvpr21.jpg">

        
    </div>

    <div id="12" class="col-sm-8">
        
<!--        <div class="title">Self-Supervised Wasserstein Pseudo-Labeling for Semi-Supervised Image Classification</div>-->
        <div class="title"> <a href="https://openaccess.thecvf.com/content/CVPR2021/html/Taherkhani_Self-Supervised_Wasserstein_Pseudo-Labeling_for_Semi-Supervised_Image_Classification_CVPR_2021_paper.html">Self-Supervised Wasserstein Pseudo-Labeling for Semi-Supervised Image Classification</a> </div>
        <div class="author">
            
            
            
            
            

            
            
            
            
            Taherkhani, Fariborz,
            
            
            
            
            
            
            
            
            

            
            
            
            
            Dabouei, Ali,
            
            
            
            
            
            
            
            
            

            
            
            
            
            Soleymani, Sobhan,
            
            
            
            
            
            
            
            
            

            
            
            
            
            Dawson, Jeremy,
            
            
            
            
            
            
            
            
            

            
            
            
            
            and Nasrabadi, Nasser M.
            
            
            
            
            
        </div>

        <div class="periodical">
            
            <em>In CVPR</em>
            
            
            2021
            
        </div>
        

        <div class="links">
            
            <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
            
            
            
            
            
            
            <a href="https://openaccess.thecvf.com/content/CVPR2021/papers/Taherkhani_Self-Supervised_Wasserstein_Pseudo-Labeling_for_Semi-Supervised_Image_Classification_CVPR_2021_paper.pdf" class="btn btn-sm z-depth-0" role="button" target="_blank">PDF</a>
            
            
            
            
            
            
            
            
        </div>

        <!-- Hidden abstract block -->
        
        <div class="abstract hidden">
            <p>The goal is to use Wasserstein metric to provide pseudo labels for the unlabeled images to train a Convolutional Neural Networks (CNN) in a Semi-Supervised Learning (SSL) manner for the classification task. The basic premise in our method is that the discrepancy between two discrete empirical measures (e.g., clusters) which come from the same or similar distribution is expected to be less than the case where these measures come from completely two different distributions. In our proposed method, we first pre-train our CNN using a self-supervised learning method to make a cluster assumption on the unlabeled images. Next, inspired by the Wasserstein metric which considers the geometry of the metric space to provide a natural notion of similarity between discrete empirical measures, we leverage it to cluster the unlabeled images and then match the clusters to their similar class of labeled images to provide a pseudo label for the data within each cluster. We have evaluated and compared our method with state-of-the-art SSL methods on the standard datasets to demonstrate its effectiveness.</p>
        </div>
        

        <!-- Hidden bibtex block -->
        
    </div>
</div>
</li>
<li><div class="row">
    <div class="col-sm-2 abbr" style="height:100%;">
        <!--  -->
        <!--    -->
        <!--    <abbr class="badge">WACV</abbr>-->
        <!--    -->
        <!--  -->
        
<!--        <img src="assets/img/sobhanwacv.jpg">-->
         <img class="img-fluid z-depth-1 rounded" src="assets/img/sobhanwacv.jpg">

        
    </div>

    <div id="11" class="col-sm-8">
        
<!--        <div class="title">Mutual Information Maximization on Disentangled Representations for Differential Morph Detection</div>-->
        <div class="title"> <a href="https://openaccess.thecvf.com/content/WACV2021/html/Soleymani_Mutual_Information_Maximization_on_Disentangled_Representations_for_Differential_Morph_Detection_WACV_2021_paper.html">Mutual Information Maximization on Disentangled Representations for Differential Morph Detection</a> </div>
        <div class="author">
            
            
            
            
            

            
            
            
            
            Soleymani, Sobhan,
            
            
            
            
            
            
            
            
            

            
            
            
            
            Dabouei, Ali,
            
            
            
            
            
            
            
            
            

            
            
            
            
            Taherkhani, Fariborz,
            
            
            
            
            
            
            
            
            

            
            
            
            
            Dawson, Jeremy,
            
            
            
            
            
            
            
            
            

            
            
            
            
            and Nasrabadi, Nasser M.
            
            
            
            
            
        </div>

        <div class="periodical">
            
            <em>In WACV</em>
            
            
            2021
            
        </div>
        

        <div class="links">
            
            <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
            
            
            
            
            
            
            <a href="https://www.ecva.net/papers/eccv_2020/papers_ECCV/papers/123490494.pdf" class="btn btn-sm z-depth-0" role="button" target="_blank">PDF</a>
            
            
            
            
            
            
            
            
        </div>

        <!-- Hidden abstract block -->
        
        <div class="abstract hidden">
            <p>In this paper, we present a novel differential morph detection framework, utilizing landmark and appearance disentanglement. In our framework, the face image is represented in the embedding domain using two disentangled but complementary representations. The network is trained by triplets of face images, in which the intermediate image inherits the landmarks from one image and the appearance from the other image. This initially trained network is further trained for each dataset using contrastive representations. We demonstrate that, by employing appearance and landmark disentanglement, the proposed framework can provide state-of-the-art differential morph detection performance. This functionality is achieved by the using distances in landmark, appearance, and ID domains. The performance of the proposed framework is evaluated using three morph datasets generated with different methodologies.</p>
        </div>
        

        <!-- Hidden bibtex block -->
        
    </div>
</div>
</li>
<li><div class="row">
    <div class="col-sm-2 abbr" style="height:100%;">
        <!--  -->
        <!--    -->
        <!--    <abbr class="badge">ECCV</abbr>-->
        <!--    -->
        <!--  -->
        
<!--        <img src="assets/img/fariborzeccv.jpg">-->
         <img class="img-fluid z-depth-1 rounded" src="assets/img/fariborzeccv.jpg">

        
    </div>

    <div id="10" class="col-sm-8">
        
<!--        <div class="title">Transporting Labels via Hierarchical Optimal Transport for Semi-Supervised Learning</div>-->
        <div class="title"> <a href="https://www.ecva.net/papers/eccv_2020/papers_ECCV/html/2402_ECCV_2020_paper.php">Transporting Labels via Hierarchical Optimal Transport for Semi-Supervised Learning</a> </div>
        <div class="author">
            
            
            
            
            

            
            
            
            
            Taherkhani, Fariborz,
            
            
            
            
            
            
            
            
            

            
            
            
            
            Dabouei, Ali,
            
            
            
            
            
            
            
            
            

            
            
            
            
            Soleymani, Sobhan,
            
            
            
            
            
            
            
            
            

            
            
            
            
            Dawson, Jeremy,
            
            
            
            
            
            
            
            
            

            
            
            
            
            and Nasrabadi, Nasser M.
            
            
            
            
            
        </div>

        <div class="periodical">
            
            <em>In ECCV</em>
            
            
            2020
            
        </div>
        

        <div class="links">
            
            <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
            
            
            
            
            
            
            <a href="https://www.ecva.net/papers/eccv_2020/papers_ECCV/papers/123490494.pdf" class="btn btn-sm z-depth-0" role="button" target="_blank">PDF</a>
            
            
            
            
            
            
            
            
        </div>

        <!-- Hidden abstract block -->
        
        <div class="abstract hidden">
            <p>Semi-Supervised Learning (SSL) based on Convolutional Neural Networks (CNNs) have recently been proven as powerful tools for standard tasks such as image classification when there is not a sufficient amount of labeled data available during the training. In this work, we consider the general setting of the SSL problem for image classification, where the labeled and unlabeled data come from the same underlying distribution. We propose a new SSL method that adopts a hierarchical Optimal Transport (OT) technique to find a mapping from empirical unlabeled measures to corresponding labeled measures by leveraging the minimum amount of transportation cost in the label space. Based on this mapping, pseudo-labels for the unlabeled data are inferred, which are then used along with the labeled data for training the CNN. We evaluated and compared our method with state-of-the-art SSL approaches on standard datasets to demonstrate the superiority of our SSL method.</p>
        </div>
        

        <!-- Hidden bibtex block -->
        
    </div>
</div>
</li>
<li><div class="row">
    <div class="col-sm-2 abbr" style="height:100%;">
        <!--  -->
        <!--    -->
        <!--    <abbr class="badge">CVPR</abbr>-->
        <!--    -->
        <!--  -->
        
<!--        <img src="assets/img/gpmr.jpg">-->
         <img class="img-fluid z-depth-1 rounded" src="assets/img/gpmr.jpg">

        
    </div>

    <div id="9" class="col-sm-8">
        
<!--        <div class="title">Exploiting Joint Robustness to Adversarial Perturbations</div>-->
        <div class="title"> <a href="https://openaccess.thecvf.com/content_CVPR_2020/html/Dabouei_Exploiting_Joint_Robustness_to_Adversarial_Perturbations_CVPR_2020_paper.html">Exploiting Joint Robustness to Adversarial Perturbations</a> </div>
        <div class="author">
            
            
            
            
            

            
            
            
            
            Dabouei, Ali,
            
            
            
            
            
            
            
            
            

            
            
            
            
            Soleymani, Sobhan,
            
            
            
            
            
            
            
            
            

            
            
            
            
            Taherkhani, Fariborz,
            
            
            
            
            
            
            
            
            

            
            
            
            
            Dawson, Jeremy,
            
            
            
            
            
            
            
            
            

            
            
            
            
            and Nasrabadi, Nasser M.
            
            
            
            
            
        </div>

        <div class="periodical">
            
            <em>In CVPR</em>
            
            
            2020
            
        </div>
        

        <div class="links">
            
            <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
            
            
            
            
            
            
            <a href="https://openaccess.thecvf.com/content_CVPR_2020/papers/Dabouei_Exploiting_Joint_Robustness_to_Adversarial_Perturbations_CVPR_2020_paper.pdf" class="btn btn-sm z-depth-0" role="button" target="_blank">PDF</a>
            
            
            
            
            
            
            
            
        </div>

        <!-- Hidden abstract block -->
        
        <div class="abstract hidden">
            <p>Recently, ensemble models have demonstrated empirical capabilities to alleviate the adversarial vulnerability. In this paper, we exploit first-order interactions within ensembles to formalize a reliable and practical defense. We introduce a scenario of interactions that certifiably improves the robustness according to the size of the ensemble, the diversity of the gradient directions, and the balance of the memberâ€™s contribution to the robustness. We present a joint gradient phase and magnitude regularization (GPMR) as a vigorous approach to impose the desired scenario of interactions among members of the ensemble. Through extensive experiments, including gradient-based and gradient-free evaluations on several datasets and network architectures, we validate the practical effectiveness of the proposed approach compared to the previous methods. Furthermore, we demonstrate that GPMR is orthogonal to other defense strategies developed for single classifiers and their combination can further improve the robustness of ensembles.</p>
        </div>
        

        <!-- Hidden bibtex block -->
        
    </div>
</div>
</li>
<li><div class="row">
    <div class="col-sm-2 abbr" style="height:100%;">
        <!--  -->
        <!--    -->
        <!--    <abbr class="badge">WACV</abbr>-->
        <!--    -->
        <!--  -->
        
<!--        <img src="assets/img/smoothfool.jpg">-->
         <img class="img-fluid z-depth-1 rounded" src="assets/img/smoothfool.jpg">

        
    </div>

    <div id="8" class="col-sm-8">
        
<!--        <div class="title">SmoothFool: An Efficient Framework for Computing Smooth Adversarial Perturbations</div>-->
        <div class="title"> <a href="https://openaccess.thecvf.com/content_WACV_2020/html/Dabouei_SmoothFool_An_Efficient_Framework_for_Computing_Smooth_Adversarial_Perturbations_WACV_2020_paper.html">SmoothFool: An Efficient Framework for Computing Smooth Adversarial Perturbations</a> </div>
        <div class="author">
            
            
            
            
            

            
            
            
            
            Dabouei, Ali,
            
            
            
            
            
            
            
            
            

            
            
            
            
            Soleymani, Sobhan,
            
            
            
            
            
            
            
            
            

            
            
            
            
            Taherkhani, Fariborz,
            
            
            
            
            
            
            
            
            

            
            
            
            
            Dawson, Jeremy,
            
            
            
            
            
            
            
            
            

            
            
            
            
            and Nasrabadi, Nasser M.
            
            
            
            
            
        </div>

        <div class="periodical">
            
            <em>In WACV</em>
            
            
            2020
            
        </div>
        

        <div class="links">
            
            <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
            
            
            
            
            
            
            <a href="https://openaccess.thecvf.com/content_WACV_2020/papers/Dabouei_SmoothFool_An_Efficient_Framework_for_Computing_Smooth_Adversarial_Perturbations_WACV_2020_paper.pdf" class="btn btn-sm z-depth-0" role="button" target="_blank">PDF</a>
            
            
            
            
            
            <a href="https://github.com/sbnsl/SmoothFool" class="btn btn-sm z-depth-0" role="button" target="_blank">Code</a>
            
            
            
            
        </div>

        <!-- Hidden abstract block -->
        
        <div class="abstract hidden">
            <p>Deep neural networks are susceptible to adversarial manipulations in the input domain. The extent of vulnerability has been explored intensively in cases of l_p-bounded and l_p-minimal adversarial perturbations. However, the vulnerability of DNNs to adversarial perturbations with specific statistical properties or frequency-domain characteristics has not been sufficiently explored. In this paper, we study the smoothness of perturbations and propose SmoothFool, a general and computationally efficient framework for computing smooth adversarial perturbations. Through extensive experiments, we validate the efficacy of the proposed method for both the white-box and black-box attack scenarios. In particular, we demonstrate that: (i) there exist extremely smooth adversarial perturbations for well-established and widely used network architectures, (ii) smoothness significantly enhances the robustness of perturbations against state-of-the-art defense mechanisms, (iii) smoothness improves the transferability of adversarial perturbations across both data points and network architectures, and (iv) class categories exhibit a variable range of susceptibility to smooth perturbations. Our results suggest that smooth APs can play a significant role in exploring the vulnerability extent of DNNs to adversarial examples..</p>
        </div>
        

        <!-- Hidden bibtex block -->
        
    </div>
</div>
</li>
<li><div class="row">
    <div class="col-sm-2 abbr" style="height:100%;">
        <!--  -->
        <!--    -->
        <!--    <abbr class="badge">WACV</abbr>-->
        <!--    -->
        <!--  -->
        
<!--        <img src="assets/img/boost.jpg">-->
         <img class="img-fluid z-depth-1 rounded" src="assets/img/boost.jpg">

        
    </div>

    <div id="7" class="col-sm-8">
        
<!--        <div class="title">Boosting Deep Face Recognition via Disentangling Appearance and Geometry</div>-->
        <div class="title"> <a href="https://openaccess.thecvf.com/content_WACV_2020/html/Dabouei_Boosting_Deep_Face_Recognition_via_Disentangling_Appearance_and_Geometry_WACV_2020_paper.html">Boosting Deep Face Recognition via Disentangling Appearance and Geometry</a> </div>
        <div class="author">
            
            
            
            
            

            
            
            
            
            Dabouei, Ali,
            
            
            
            
            
            
            
            
            

            
            
            
            
            Taherkhani, Fariborz,
            
            
            
            
            
            
            
            
            

            
            
            
            
            Soleymani, Sobhan,
            
            
            
            
            
            
            
            
            

            
            
            
            
            Dawson, Jeremy,
            
            
            
            
            
            
            
            
            

            
            
            
            
            and Nasrabadi, Nasser M.
            
            
            
            
            
        </div>

        <div class="periodical">
            
            <em>In WACV</em>
            
            
            2020
            
        </div>
        

        <div class="links">
            
            <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
            
            
            
            
            
            
            <a href="https://openaccess.thecvf.com/content_WACV_2020/papers/Dabouei_Boosting_Deep_Face_Recognition_via_Disentangling_Appearance_and_Geometry_WACV_2020_paper.pdf" class="btn btn-sm z-depth-0" role="button" target="_blank">PDF</a>
            
            
            
            
            
            
            
            
        </div>

        <!-- Hidden abstract block -->
        
        <div class="abstract hidden">
            <p>In this paper, we propose a framework for disentangling the appearance and geometry representations in the face recognition task. To provide supervision for this aim, we generate geometrically identical faces by incorporating spatial transformations. We demonstrate that the proposed approach enhances the performance of deep face recognition models by assisting the training process in two ways. First, it enforces the early and intermediate convolutional layers to learn more representative features that satisfy the properties of disentangled embeddings. Second, it augments the training set by altering faces geometrically. Through extensive experiments, we demonstrate that integrating the proposed approach into state-of-the-art face recognition methods effectively improves their performance on challenging datasets, such as LFW, YTF, and MegaFace. Both theoretical and practical aspects of the method are analyzed rigorously by concerning ablation studies and knowledge transfer tasks. Furthermore, we show that the knowledge leaned by the proposed method can favor other face-related tasks, such as attribute prediction.</p>
        </div>
        

        <!-- Hidden bibtex block -->
        
    </div>
</div>
</li>
<li><div class="row">
    <div class="col-sm-2 abbr" style="height:100%;">
        <!--  -->
        <!--    -->
        <!--    <abbr class="badge">WACV</abbr>-->
        <!--    -->
        <!--  -->
        
<!--        <img src="assets/img/mehdi1.jpg">-->
         <img class="img-fluid z-depth-1 rounded" src="assets/img/mehdi1.jpg">

        
    </div>

    <div id="6" class="col-sm-8">
        
<!--        <div class="title">Robust Facial Landmark Detection via Aggregation on Geometrically Manipulated Faces</div>-->
        <div class="title"> <a href="https://openaccess.thecvf.com/content_WACV_2020/html/Iranmanesh_Robust_Facial_Landmark_Detection_via_Aggregation_on_Geometrically_Manipulated_Faces_WACV_2020_paper.html">Robust Facial Landmark Detection via Aggregation on Geometrically Manipulated Faces</a> </div>
        <div class="author">
            
            
            
            
            

            
            
            
            
            Iranmanesh, Seyed Mehdi,
            
            
            
            
            
            
            
            
            

            
            
            
            
            Dabouei, Ali,
            
            
            
            
            
            
            
            
            

            
            
            
            
            Soleymani, Sobhan,
            
            
            
            
            
            
            
            
            

            
            
            
            
            Kazemi, Hadi,
            
            
            
            
            
            
            
            
            

            
            
            
            
            and Nasrabadi, Nasser
            
            
            
            
            
        </div>

        <div class="periodical">
            
            <em>In WACV</em>
            
            
            2020
            
        </div>
        

        <div class="links">
            
            <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
            
            
            
            
            
            
            <a href="https://openaccess.thecvf.com/content_ICCV_2019/papers/Taherkhani_A_Weakly_Supervised_Fine_Label_Classifier_Enhanced_by_Coarse_Supervision_ICCV_2019_paper.pdf" class="btn btn-sm z-depth-0" role="button" target="_blank">PDF</a>
            
            
            
            
            
            
            
            
        </div>

        <!-- Hidden abstract block -->
        
        <div class="abstract hidden">
            <p>In this work, we present a practical approach to the problem of facial landmark detection. The proposed method can deal with large shape and appearance variations under the rich shape deformation. To handle the shape variations we equip our method with the aggregation of manipulated face images. The proposed framework generates different manipulated faces using only one given face image. The approach utilizes the fact that small but carefully crafted geometric manipulation in the input domain can fool deep face recognition models. We propose three different approaches to generate manipulated faces in which two of them perform the manipulations via adversarial attacks and the other one uses known transformations. Aggregating the manipulated faces provides a more robust landmark detection approach which is able to capture more important deformations and variations of the face shapes. Our approach is demonstrated its superiority compared to the state-of-the-art method on benchmark datasets AFLW, 300-W, and COFW.</p>
        </div>
        

        <!-- Hidden bibtex block -->
        
    </div>
</div>
</li>
<li><div class="row">
    <div class="col-sm-2 abbr" style="height:100%;">
        <!--  -->
        <!--    -->
        <!--    <abbr class="badge">ICB</abbr>-->
        <!--    -->
        <!--  -->
        
<!--        <img src="assets/img/ICB2019.png">-->
         <img class="img-fluid z-depth-1 rounded" src="assets/img/ICB2019.png">

        
    </div>

    <div id="5" class="col-sm-8">
        
<!--        <div class="title">Adversarial Examples to Fool Iris Recognition Systems</div>-->
        <div class="title"> <a href="https://ieeexplore.ieee.org/abstract/document/8987389">Adversarial Examples to Fool Iris Recognition Systems</a> </div>
        <div class="author">
            
            
            
            
            

            
            
            
            
            Soleymani, Sobhan,
            
            
            
            
            
            
            
            
            

            
            
            
            
            Dabouei, Ali,
            
            
            
            
            
            
            
            
            

            
            
            
            
            Dawson, Jeremy,
            
            
            
            
            
            
            
            
            

            
            
            
            
            and Nasrabadi, Nasser M.
            
            
            
            
            
        </div>

        <div class="periodical">
            
            <em>In ICB</em>
            
            
            2019
            
        </div>
        

        <div class="links">
            
            <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
            
            
            <a href="http://arxiv.org/abs/1906.09300" class="btn btn-sm z-depth-0" role="button" target="_blank">arXiv</a>
            
            
            
            
            
            
            
            <a href="https://github.com/sbnsl/Adversarial-Iris-Generation" class="btn btn-sm z-depth-0" role="button" target="_blank">Code</a>
            
            
            
            
        </div>

        <!-- Hidden abstract block -->
        
        <div class="abstract hidden">
            <p>Adversarial examples have recently proven to be able to fool deep learning methods by adding carefully crafted small perturbation to the input space image. In this paper, we study the possibility of generating adversarial examples for code-based iris recognition systems. Since generating adversarial examples requires back-propagation of the adversarial loss, conventional filter bank-based iris-code generation frameworks cannot be employed in such a setup. Therefore, to compensate for this shortcoming, we propose to train a deep auto-encoder surrogate network to mimic the conventional iris code generation procedure. This trained surrogate network is then deployed to generate the adversarial examples using the iterative gradient sign method algorithm. We consider non-targeted and targeted attacks through three attack scenarios. Considering these attacks, we study the possibility of fooling an iris recognition system in white-box and black-box frameworks.</p>
        </div>
        

        <!-- Hidden bibtex block -->
        
    </div>
</div>
</li>
<li><div class="row">
    <div class="col-sm-2 abbr" style="height:100%;">
        <!--  -->
        <!--    -->
        <!--    <abbr class="badge">NeurIPS</abbr>-->
        <!--    -->
        <!--  -->
        
<!--        <img src="assets/img/nips2018.png">-->
         <img class="img-fluid z-depth-1 rounded" src="assets/img/nips2018.png">

        
    </div>

    <div id="4" class="col-sm-8">
        
<!--        <div class="title">Unsupervised Image-to-Image Translation Using Domain-Specific Variational Information Bound</div>-->
        <div class="title"> <a href="https://proceedings.neurips.cc/paper/2018/hash/c7c46d4baf816bfb07c7f3bf96d88544-Abstract.html">Unsupervised Image-to-Image Translation Using Domain-Specific Variational Information Bound</a> </div>
        <div class="author">
            
            
            
            
            

            
            
            
            
            Kazemi, Hadi,
            
            
            
            
            
            
            
            
            

            
            
            
            
            Soleymani, Sobhan,
            
            
            
            
            
            
            
            
            

            
            
            
            
            Taherkhani, Fariborz,
            
            
            
            
            
            
            
            
            

            
            
            
            
            Iranmanesh, Seyed Mehdi,
            
            
            
            
            
            
            
            
            

            
            
            
            
            and Nasrabadi, Nasser M.
            
            
            
            
            
        </div>

        <div class="periodical">
            
            <em>In NeurIPS</em>
            
            
            2018
            
        </div>
        

        <div class="links">
            
            <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
            
            
            
            
            
            
            <a href="https://proceedings.neurips.cc/paper/2018/file/c7c46d4baf816bfb07c7f3bf96d88544-Paper.pdf" class="btn btn-sm z-depth-0" role="button" target="_blank">PDF</a>
            
            
            
            
            
            
            
            
        </div>

        <!-- Hidden abstract block -->
        
        <div class="abstract hidden">
            <p>Unsupervised image-to-image translation is a class of computer vision problems which aims at modeling conditional distribution of images in the target domain, given a set of unpaired images in the source and target domains. An image in the source domain might have multiple representations in the target domain. Therefore, ambiguity in modeling of the conditional distribution arises, specially when the images in the source and target domains come from different modalities. Current approaches mostly rely on simplifying assumptions to map both domains into a shared-latent space. Consequently, they are only able to model the domain-invariant information between the two modalities. These approaches usually fail to model domain-specific information which has no representation in the target domain. In this work, we propose an unsupervised image-to-image translation framework which maximizes a domain-specific variational information bound and learns the target domain-invariant representation of the two domain. The proposed framework makes it possible to map a single source image into multiple images in the target domain, utilizing several target domain-specific codes sampled randomly from the prior distribution, or extracted from reference images.</p>
        </div>
        

        <!-- Hidden bibtex block -->
        
    </div>
</div>
</li>
<li><div class="row">
    <div class="col-sm-2 abbr" style="height:100%;">
        <!--  -->
        <!--    -->
        <!--    <abbr class="badge">BTAS</abbr>-->
        <!--    -->
        <!--  -->
        
<!--        <img src="assets/img/BTAS2018.png">-->
         <img class="img-fluid z-depth-1 rounded" src="assets/img/BTAS2018.png">

        
    </div>

    <div id="3" class="col-sm-8">
        
<!--        <div class="title">Prosodic-Enhanced Siamese Convolutional Neural Networks for Cross-Device Text-Independent Speaker Verification</div>-->
        <div class="title"> <a href="https://ieeexplore.ieee.org/abstract/document/8698585">Prosodic-Enhanced Siamese Convolutional Neural Networks for Cross-Device Text-Independent Speaker Verification</a> </div>
        <div class="author">
            
            
            
            
            

            
            
            
            
            Soleymani, Sobhan,
            
            
            
            
            
            
            
            
            

            
            
            
            
            Dabouei, Ali,
            
            
            
            
            
            
            
            
            

            
            
            
            
            Iranmanesh, Seyed Mehdi,
            
            
            
            
            
            
            
            
            

            
            
            
            
            Kazemi, Hadi,
            
            
            
            
            
            
            
            
            

            
            
            
            
            Dawson, Jeremy,
            
            
            
            
            
            
            
            
            

            
            
            
            
            and Nasrabadi, Nasser M.
            
            
            
            
            
        </div>

        <div class="periodical">
            
            <em>In BTAS</em>
            
            
            2018
            
        </div>
        

        <div class="links">
            
            <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
            
            
            <a href="http://arxiv.org/abs/1808.01026" class="btn btn-sm z-depth-0" role="button" target="_blank">arXiv</a>
            
            
            
            
            
            
            
            <a href="https://github.com/sbnsl/cross-device-text-independent-speaker-verification" class="btn btn-sm z-depth-0" role="button" target="_blank">Code</a>
            
            
            
            
        </div>

        <!-- Hidden abstract block -->
        
        <div class="abstract hidden">
            <p>In this paper a novel cross-device text-independent speaker verification architecture is proposed. Majority of the state-of-the-art deep architectures that are used for speaker verification tasks consider Mel-frequency cepstral coefficients. In contrast, our proposed Siamese convolutional neural network architecture uses Mel-frequency spectrogram coefficients to benefit from the dependency of the adjacent spectro-temporal features. Moreover, although spectro-temporal features have proved to be highly reliable in speaker verification models, they only represent some aspects of short-term acoustic level traits of the speakerâ€™s voice. However, the human voice consists of several linguistic levels such as acoustic, lexicon, prosody, and phonetics, that can be utilized in speaker verification models. To compensate for these inherited shortcomings in spectro-temporal features, we propose to enhance the proposed Siamese convolutional neural network architecture by deploying a multilayer perceptron network to incorporate the prosodic, jitter, and shimmer features. The proposed end-to-end verification architecture performs feature extraction and verification simultaneously. This proposed architecture displays significant improvement over classical signal processing approaches and deep algorithms for forensic cross-device speaker verification.</p>
        </div>
        

        <!-- Hidden bibtex block -->
        
    </div>
</div>
</li>
<li><div class="row">
    <div class="col-sm-2 abbr" style="height:100%;">
        <!--  -->
        <!--    -->
        <!--    <abbr class="badge">WACV</abbr>-->
        <!--    -->
        <!--  -->
        
<!--        <img src="assets/img/geo1.jpg">-->
         <img class="img-fluid z-depth-1 rounded" src="assets/img/geo1.jpg">

        
    </div>

    <div id="2" class="col-sm-8">
        
<!--        <div class="title">Fast Geometrically-Perturbed Adversarial Faces</div>-->
        <div class="title"> <a href="https://arxiv.org/pdf/1809.08999.pdf">Fast Geometrically-Perturbed Adversarial Faces</a> </div>
        <div class="author">
            
            
            
            
            

            
            
            
            
            Dabouei, Ali,
            
            
            
            
            
            
            
            
            

            
            
            
            
            Soleymani, Sobhan,
            
            
            
            
            
            
            
            
            

            
            
            
            
            Dawson, Jeremy,
            
            
            
            
            
            
            
            
            

            
            
            
            
            and Nasrabadi, Nasser M.
            
            
            
            
            
        </div>

        <div class="periodical">
            
            <em>In WACV</em>
            
            
            2019
            
        </div>
        

        <div class="links">
            
            <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
            
            
            <a href="http://arxiv.org/abs/1809.08999" class="btn btn-sm z-depth-0" role="button" target="_blank">arXiv</a>
            
            
            
            
            
            
            
            <a href="https://github.com/sbnsl/FLM" class="btn btn-sm z-depth-0" role="button" target="_blank">Code</a>
            
            
            
            
        </div>

        <!-- Hidden abstract block -->
        
        <div class="abstract hidden">
            <p>The state-of-the-art performance of deep learning algorithms has led to a considerable increase in the utilization of machine learning in security-sensitive and critical applications. However, it has recently been shown that a small and carefully crafted perturbation in the input space can completely fool a deep model. In this study, we explore the extent to which face recognition systems are vulnerable to geometrically-perturbed adversarial faces. We propose a fast landmark manipulation method for generating adversarial faces, which is approximately 200 times faster than the previous geometric attacks and obtains 99.86% success rate on the state-of-the-art face recognition models. To further force the generated samples to be natural, we introduce a second attack constrained on the semantic structure of the face which has the half speed of the first attack with the success rate of 99.96%. Both attacks are extremely robust against the state-of-the-art defense methods with the success rate of equal or greater than 53.59%.</p>
        </div>
        

        <!-- Hidden bibtex block -->
        
    </div>
</div>
</li>
<li><div class="row">
    <div class="col-sm-2 abbr" style="height:100%;">
        <!--  -->
        <!--    -->
        <!--    <abbr class="badge">ICPR</abbr>-->
        <!--    -->
        <!--  -->
        
<!--        <img src="assets/img/ICPR2018.png">-->
         <img class="img-fluid z-depth-1 rounded" src="assets/img/ICPR2018.png">

        
    </div>

    <div id="1" class="col-sm-8">
        
<!--        <div class="title">Multi-Level Feature Abstraction from Convolutional Neural Networks for Multimodal Biometric Identification</div>-->
        <div class="title"> <a href="https://ieeexplore.ieee.org/abstract/document/8545061">Multi-Level Feature Abstraction from Convolutional Neural Networks for Multimodal Biometric Identification</a> </div>
        <div class="author">
            
            
            
            
            

            
            
            
            
            Soleymani, Sobhan,
            
            
            
            
            
            
            
            
            

            
            
            
            
            Dabouei, Ali,
            
            
            
            
            
            
            
            
            

            
            
            
            
            Kazemi, Hadi,
            
            
            
            
            
            
            
            
            

            
            
            
            
            Dawson, Jeremy,
            
            
            
            
            
            
            
            
            

            
            
            
            
            and Nasrabadi, Nasser M.
            
            
            
            
            
        </div>

        <div class="periodical">
            
            <em>In ICPR</em>
            
            
            2018
            
        </div>
        

        <div class="links">
            
            <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
            
            
            <a href="http://arxiv.org/abs/1807.01332" class="btn btn-sm z-depth-0" role="button" target="_blank">arXiv</a>
            
            
            
            
            
            
            
            
            
            
        </div>

        <!-- Hidden abstract block -->
        
        <div class="abstract hidden">
            <p>In this paper, we propose a deep multimodal fusion network to fuse multiple modalities (face, iris, and fingerprint) for person identification. The proposed deep multimodal fusion algorithm consists of multiple streams of modality-specific Convolutional Neural Networks (CNNs), which are jointly optimized at multiple feature abstraction levels. Multiple features are extracted at several different convolutional layers from each modality-specific CNN for joint feature fusion, optimization,and classification. Features extracted at different convolutional layers of a modality-specific CNN represent the input at several different levels of abstract representations. We demonstrate that an efficient multimodal classification can be accomplished with a significant reduction in the number of network parameters by exploiting these multi-level abstract representations extracted from all the modality-specific CNNs. We demonstrate an increase in multimodal person identification performance by utilizing the proposed multi-level feature abstract representations in our multimodal fusion, rather than using only the features from the last layer of each modality-specific CNNs. We show that our deep multi-modal CNNs with multimodal fusion at several different feature level abstraction can significantly outperform the unimodal representation accuracy. We also demonstrate that the joint optimization of all the modality-specific CNNs excels the score and decision level fusions of independently optimized CNNs.</p>
        </div>
        

        <!-- Hidden bibtex block -->
        
    </div>
</div>
</li></ol>
</div>

    


  </article>

</div>

    </div>

    <!-- Footer -->

    
<footer class="fixed-bottom">
  <div class="container mt-0">
    &copy; Copyright 2022 Sobhan  Soleymani.
    Powered by <a href="http://jekyllrb.com/" target="_blank">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank">GitHub Pages</a>. Photos from <a href="https://unsplash.com" target="_blank">Unsplash</a>.

    
    
  </div>
</footer>



  </body>

  <!-- Bootsrap & MDB scripts -->
<script src="https://cdnjs.cloudflare.com/ajax/libs/popper.js/2.4.4/umd/popper.min.js" integrity="sha512-eUQ9hGdLjBjY3F41CScH3UX+4JDSI9zXeroz7hJ+RteoCaY+GP/LDoM8AO+Pt+DRFw3nXqsjh9Zsts8hnYv8/A==" crossorigin="anonymous"></script>
<script src="https://stackpath.bootstrapcdn.com/bootstrap/4.5.2/js/bootstrap.min.js" integrity="sha512-M5KW3ztuIICmVIhjSqXe01oV2bpe248gOxqmlcYrEzAvws7Pw3z6BK0iGbrwvdrUQUhi3eXgtxp5I8PDo9YfjQ==" crossorigin="anonymous"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/mdbootstrap/4.19.1/js/mdb.min.js" integrity="sha512-Mug9KHKmroQFMLm93zGrjhibM2z2Obg9l6qFG2qKjXEXkMp/VDkI4uju9m4QKPjWSwQ6O2qzZEnJDEeCw0Blcw==" crossorigin="anonymous"></script>

  
<!-- Mansory & imagesLoaded -->
<script defer src="https://unpkg.com/masonry-layout@4/dist/masonry.pkgd.min.js"></script>
<script defer src="https://unpkg.com/imagesloaded@4/imagesloaded.pkgd.min.js"></script>
<script defer src="/assets/js/mansory.js" type="text/javascript"></script>


  


<!-- Medium Zoom JS -->
<script src="https://cdn.jsdelivr.net/npm/medium-zoom@1.0.6/dist/medium-zoom.min.js" integrity="sha256-EdPgYcPk/IIrw7FYeuJQexva49pVRZNmt3LculEr7zM=" crossorigin="anonymous"></script>
<script src="/assets/js/zoom.js"></script>


<!-- Load Common JS -->
<script src="/assets/js/common.js"></script>


</html>
