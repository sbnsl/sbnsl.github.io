---
---
@string{TBIOM = {TBIOM}}
@string{CVPR = {CVPR}}
@string{ICLR = {ICLR}}
@string{ICCV = {ICCV}}
@string{WACV = {WACV}}
@string{BMVC = {BMVC}}
@string{ECCV = {ECCV}}
@string{NeurIPS = {NeurIPS}}
@string{ICB = {ICB}}
@string{ICPR = {ICPR}}
@string{ICIP = {ICIP}}
@string{BTAS = {BTAS}}

@inproceedings{13,
 abbr={TBIOM},
 mainlink = {https://ieeexplore.ieee.org/abstract/document/9631949},
  img = {assets/img/TBIOM_Fig02.png},
  author    = {Sobhan Soleymani and Ali Dabouei and Fariborz Taherkhani Seyed Mehdi Iranmanesh and Jeremy Dawson and Nasser M. Nasrabadi},
  title     = {Quality-Aware Multimodal Biometric Recognition},
  booktitle = TBIOM,
  year      = {2021},
  pdf       = {https://arxiv.org/pdf/2112.05827.pdf},
  abstract = {We present a quality-aware multimodal recognition framework that combines representations from multiple biometric traits with varying quality and number of samples to achieve increased recognition accuracy by extracting complimentary identification information based on the quality of the samples. We develop a quality-aware framework for fusing representations of input modalities by weighting their importance using quality scores estimated in a weakly-supervised fashion. This framework utilizes two fusion blocks, each represented by a set of quality-aware and aggregation networks. In addition to architecture modifications, we propose two task-specific loss functions: multimodal separability loss and multimodal compactness loss. The first loss assures that the representations of modalities for a class have comparable magnitudes to provide a better quality estimation, while the multimodal representations of different classes are distributed to achieve maximum discrimination in the embedding space. The second loss, which is considered to regularize the network weights, improves the generalization performance by regularizing the framework. We evaluate the performance by considering three multimodal datasets consisting of face, iris, and fingerprint modalities. The efficacy of the framework is demonstrated through comparison with the state-of-the-art algorithms. In particular, our framework outperforms the rank- and score-level fusion of modalities of BIOMDATA by more than 30% for true acceptance rate at false acceptance rate of 10^-4.},
        selected={true}
}

@inproceedings{12,
 abbr={CVPR},
 mainlink = {https://openaccess.thecvf.com/content/CVPR2021/html/Dabouei_SuperMix_Supervising_the_Mixing_Data_Augmentation_CVPR_2021_paper.html},
  img = {assets/img/supermix.jpg},
  author    = {Ali Dabouei and Sobhan Soleymani and Fariborz Taherkhani and Nasser M. Nasrabadi},
  title     = {SuperMix: Supervising the Mixing Data Augmentation},
  booktitle = CVPR,
  year      = {2021},
  pdf       = {https://openaccess.thecvf.com/content/CVPR2021/papers/Dabouei_SuperMix_Supervising_the_Mixing_Data_Augmentation_CVPR_2021_paper.pdf},
  code      = {https://github.com/sbnsl/SuperMix},
  abstract = {This paper presents a supervised mixing augmentation method termed SuperMix, which exploits the salient regions within input images to construct mixed training samples. SuperMix is designed to obtain mixed images rich in visual features and complying with realistic image priors. To enhance the efficiency of the algorithm, we develop a variant of the Newton iterative method, 65xfaster than gradient descent on this problem. We validate the effectiveness of SuperMix through extensive evaluations and ablation studies on two tasks of object classification and knowledge distillation. On the classification task, SuperMix provides comparable performance to the advanced augmentation methods, such as AutoAugment and RandAugment. In particular, combining SuperMix with RandAugment achieves 78.2% top-1 accuracy on ImageNet with ResNet50. On the distillation task, solely classifying images mixed using the teacher's knowledge achieves comparable performance to the state-of-the-art distillation methods. Furthermore, on average, incorporating mixed images into the distillation objective improves the performance by 3.4% and 3.1% on CIFAR-100 and ImageNet, respectively.},
        selected={true}
}

@inproceedings{11,
 abbr={CVPR},
  mainlink = {https://openaccess.thecvf.com/content/CVPR2021/html/Taherkhani_Self-Supervised_Wasserstein_Pseudo-Labeling_for_Semi-Supervised_Image_Classification_CVPR_2021_paper.html},
 img = {assets/img/far_cvpr21.jpg},
  author    = {Fariborz Taherkhani and Ali Dabouei and Sobhan Soleymani and Jeremy Dawson and Nasser M. Nasrabadi},
  title     = {Self-Supervised Wasserstein Pseudo-Labeling for Semi-Supervised Image Classification},
  booktitle = CVPR,
  year      = {2021},
  pdf       = {https://openaccess.thecvf.com/content/CVPR2021/papers/Taherkhani_Self-Supervised_Wasserstein_Pseudo-Labeling_for_Semi-Supervised_Image_Classification_CVPR_2021_paper.pdf},
  abstract = {The goal is to use Wasserstein metric to provide pseudo labels for the unlabeled images to train a Convolutional Neural Networks (CNN) in a Semi-Supervised Learning (SSL) manner for the classification task. The basic premise in our method is that the discrepancy between two discrete empirical measures (e.g., clusters) which come from the same or similar distribution is expected to be less than the case where these measures come from completely two different distributions. In our proposed method, we first pre-train our CNN using a self-supervised learning method to make a cluster assumption on the unlabeled images. Next, inspired by the Wasserstein metric which considers the geometry of the metric space to provide a natural notion of similarity between discrete empirical measures, we leverage it to cluster the unlabeled images and then match the clusters to their similar class of labeled images to provide a pseudo label for the data within each cluster. We have evaluated and compared our method with state-of-the-art SSL methods on the standard datasets to demonstrate its effectiveness.},
        selected={true}
}

@inproceedings{10,
 abbr={WACV},
  mainlink = {https://openaccess.thecvf.com/content/WACV2021/html/Soleymani_Mutual_Information_Maximization_on_Disentangled_Representations_for_Differential_Morph_Detection_WACV_2021_paper.html},
 img = {assets/img/sobhanwacv.jpg},
  author    = {Sobhan Soleymani and Ali Dabouei and Fariborz Taherkhani and Jeremy Dawson and Nasser M. Nasrabadi},
  title     = {Mutual Information Maximization on Disentangled Representations for Differential Morph Detection},
  booktitle = WACV,
  year      = {2021},
  pdf       = {https://www.ecva.net/papers/eccv_2020/papers_ECCV/papers/123490494.pdf},
  abstract = {In this paper, we present a novel differential morph detection framework, utilizing landmark and appearance disentanglement. In our framework, the face image is represented in the embedding domain using two disentangled but complementary representations. The network is trained by triplets of face images, in which the intermediate image inherits the landmarks from one image and the appearance from the other image. This initially trained network is further trained for each dataset using contrastive representations. We demonstrate that, by employing appearance and landmark disentanglement, the proposed framework can provide state-of-the-art differential morph detection performance. This functionality is achieved by the using distances in landmark, appearance, and ID domains. The performance of the proposed framework is evaluated using three morph datasets generated with different methodologies.},
        selected={true}
}

@inproceedings{9,
 abbr={ECCV},
  mainlink = {https://www.ecva.net/papers/eccv_2020/papers_ECCV/html/2402_ECCV_2020_paper.php},
  img = {assets/img/fariborzeccv.jpg},
  author    = {Fariborz Taherkhani and Ali Dabouei and Sobhan Soleymani and Jeremy Dawson and Nasser M. Nasrabadi},
  title     = {Transporting Labels via Hierarchical Optimal Transport for Semi-Supervised Learning},
  booktitle = ECCV,
  year      = {2020},
  pdf       = {https://www.ecva.net/papers/eccv_2020/papers_ECCV/papers/123490494.pdf},
  abstract = {Semi-Supervised Learning (SSL) based on Convolutional Neural Networks (CNNs) have recently been proven as powerful tools for standard tasks such as image classification when there is not a sufficient amount of labeled data available during the training. In this work, we consider the general setting of the SSL problem for image classification, where the labeled and unlabeled data come from the same underlying distribution. We propose a new SSL method that adopts a hierarchical Optimal Transport (OT) technique to find a mapping from empirical unlabeled measures to corresponding labeled measures by leveraging the minimum amount of transportation cost in the label space. Based on this mapping, pseudo-labels for the unlabeled data are inferred, which are then used along with the labeled data for training the CNN. We evaluated and compared our method with state-of-the-art SSL approaches on standard datasets to demonstrate the superiority of our SSL method.},
        selected={true}
}

@inproceedings{8,
 abbr={CVPR},
  mainlink = {https://openaccess.thecvf.com/content_CVPR_2020/html/Dabouei_Exploiting_Joint_Robustness_to_Adversarial_Perturbations_CVPR_2020_paper.html},
    img = {assets/img/gpmr.jpg},
  author    = {Ali Dabouei and Sobhan Soleymani and Fariborz Taherkhani and Jeremy Dawson and Nasser M. Nasrabadi},
  title     = {Exploiting Joint Robustness to Adversarial Perturbations},
  booktitle = CVPR,
  year      = {2020},
  pdf       = {https://openaccess.thecvf.com/content_CVPR_2020/papers/Dabouei_Exploiting_Joint_Robustness_to_Adversarial_Perturbations_CVPR_2020_paper.pdf},
  abstract = {Recently, ensemble models have demonstrated empirical capabilities to alleviate the adversarial vulnerability. In this paper, we exploit first-order interactions within ensembles to formalize a reliable and practical defense. We introduce a scenario of interactions that certifiably improves the robustness according to the size of the ensemble, the diversity of the gradient directions, and the balance of the member's contribution to the robustness. We present a joint gradient phase and magnitude regularization (GPMR) as a vigorous approach to impose the desired scenario of interactions among members of the ensemble. Through extensive experiments, including gradient-based and gradient-free evaluations on several datasets and network architectures, we validate the practical effectiveness of the proposed approach compared to the previous methods. Furthermore, we demonstrate that GPMR is orthogonal to other defense strategies developed for single classifiers and their combination can further improve the robustness of ensembles.},
        selected={true}
}

@inproceedings{7,
 abbr={WACV},
  mainlink = {https://openaccess.thecvf.com/content_WACV_2020/html/Dabouei_SmoothFool_An_Efficient_Framework_for_Computing_Smooth_Adversarial_Perturbations_WACV_2020_paper.html},
     img = {assets/img/smoothfool.jpg},
  author    = {Ali Dabouei and Sobhan Soleymani and Fariborz Taherkhani and Jeremy Dawson and Nasser M. Nasrabadi},
  title     = {SmoothFool: An Efficient Framework for Computing Smooth Adversarial Perturbations},
  booktitle = WACV,
  year      = {2020},
  pdf       = {https://openaccess.thecvf.com/content_WACV_2020/papers/Dabouei_SmoothFool_An_Efficient_Framework_for_Computing_Smooth_Adversarial_Perturbations_WACV_2020_paper.pdf},
  code      = {https://github.com/sbnsl/SmoothFool},
  abstract = {Deep neural networks are susceptible to adversarial manipulations in the input domain. The extent of vulnerability has been explored intensively in cases of l_p-bounded and l_p-minimal adversarial perturbations. However, the vulnerability of DNNs to adversarial perturbations with specific statistical properties or frequency-domain characteristics has not been sufficiently explored. In this paper, we study the smoothness of perturbations and propose SmoothFool, a general and computationally efficient framework for computing smooth adversarial perturbations. Through extensive experiments, we validate the efficacy of the proposed method for both the white-box and black-box attack scenarios. In particular, we demonstrate that: (i) there exist extremely smooth adversarial perturbations for well-established and widely used network architectures, (ii) smoothness significantly enhances the robustness of perturbations against state-of-the-art defense mechanisms, (iii) smoothness improves the transferability of adversarial perturbations across both data points and network architectures, and (iv) class categories exhibit a variable range of susceptibility to smooth perturbations. Our results suggest that smooth APs can play a significant role in exploring the vulnerability extent of DNNs to adversarial examples..},
        selected={true}
}

@inproceedings{6,
 abbr={WACV},
  mainlink = {https://openaccess.thecvf.com/content_WACV_2020/html/Dabouei_Boosting_Deep_Face_Recognition_via_Disentangling_Appearance_and_Geometry_WACV_2020_paper.html},
      img = {assets/img/boost.jpg},
  author    = {Ali Dabouei and Fariborz Taherkhani and Sobhan Soleymani and Jeremy Dawson and Nasser M. Nasrabadi},
  title     = {Boosting Deep Face Recognition via Disentangling Appearance and Geometry},
  booktitle = WACV,
  year      = {2020},
  pdf       = {https://openaccess.thecvf.com/content_WACV_2020/papers/Dabouei_Boosting_Deep_Face_Recognition_via_Disentangling_Appearance_and_Geometry_WACV_2020_paper.pdf},
  abstract = {In this paper, we propose a framework for disentangling the appearance and geometry representations in the face recognition task. To provide supervision for this aim, we generate geometrically identical faces by incorporating spatial transformations. We demonstrate that the proposed approach enhances the performance of deep face recognition models by assisting the training process in two ways. First, it enforces the early and intermediate convolutional layers to learn more representative features that satisfy the properties of disentangled embeddings. Second, it augments the training set by altering faces geometrically. Through extensive experiments, we demonstrate that integrating the proposed approach into state-of-the-art face recognition methods effectively improves their performance on challenging datasets, such as LFW, YTF, and MegaFace. Both theoretical and practical aspects of the method are analyzed rigorously by concerning ablation studies and knowledge transfer tasks. Furthermore, we show that the knowledge leaned by the proposed method can favor other face-related tasks, such as attribute prediction.},
        selected={true}
}

@inproceedings{5,
 abbr={WACV},
  mainlink = {https://openaccess.thecvf.com/content_WACV_2020/html/Iranmanesh_Robust_Facial_Landmark_Detection_via_Aggregation_on_Geometrically_Manipulated_Faces_WACV_2020_paper.html},
       img = {assets/img/mehdi1.jpg},
  author    = {Seyed Mehdi Iranmanesh and Ali Dabouei and Sobhan Soleymani and Hadi Kazemi and Nasser Nasrabadi},
  title     = {Robust Facial Landmark Detection via Aggregation on Geometrically Manipulated Faces},
  booktitle = WACV,
  year      = {2020},
  pdf       = {https://openaccess.thecvf.com/content_ICCV_2019/papers/Taherkhani_A_Weakly_Supervised_Fine_Label_Classifier_Enhanced_by_Coarse_Supervision_ICCV_2019_paper.pdf},
  abstract = {In this work, we present a practical approach to the problem of facial landmark detection. The proposed method can deal with large shape and appearance variations under the rich shape deformation. To handle the shape variations we equip our method with the aggregation of manipulated face images. The proposed framework generates different manipulated faces using only one given face image. The approach utilizes the fact that small but carefully crafted geometric manipulation in the input domain can fool deep face recognition models. We propose three different approaches to generate manipulated faces in which two of them perform the manipulations via adversarial attacks and the other one uses known transformations. Aggregating the manipulated faces provides a more robust landmark detection approach which is able to capture more important deformations and variations of the face shapes. Our approach is demonstrated its superiority compared to the state-of-the-art method on benchmark datasets AFLW, 300-W, and COFW.},
        selected={true}
}

@inproceedings{4,
 abbr={ICB},
  mainlink = {https://ieeexplore.ieee.org/abstract/document/8987389},
         img = {assets/img/ICB2019.png},
  author    = {Sobhan Soleymani and Ali Dabouei and Jeremy Dawson and
               Nasser M. Nasrabadi},
  title     = {Adversarial Examples to Fool Iris Recognition Systems},
  booktitle = ICB,
  year      = {2019},
  arxiv       = {1906.09300},
  code     =    {https://github.com/sbnsl/Adversarial-Iris-Generation},
  abstract = {Adversarial examples have recently proven to be able to fool deep learning methods by adding carefully crafted small perturbation to the input space image. In this paper, we study the possibility of generating adversarial examples for code-based iris recognition systems. Since generating adversarial examples requires back-propagation of the adversarial loss, conventional filter bank-based iris-code generation frameworks cannot be employed in such a setup. Therefore, to compensate for this shortcoming, we propose to train a deep auto-encoder surrogate network to mimic the conventional iris code generation procedure. This trained surrogate network is then deployed to generate the adversarial examples using the iterative gradient sign method algorithm. We consider non-targeted and targeted attacks through three attack scenarios. Considering these attacks, we study the possibility of fooling an iris recognition system in white-box and black-box frameworks.},
        selected={true}
}


@inproceedings{3,
 abbr={NeurIPS},
  mainlink = {https://proceedings.neurips.cc/paper/2018/hash/c7c46d4baf816bfb07c7f3bf96d88544-Abstract.html},
        img = {assets/img/nips2018.png},
  author    = {Hadi Kazemi and Sobhan Soleymani and Fariborz Taherkhani and Seyed Mehdi Iranmanesh and Nasser M. Nasrabadi},
  title     = {Unsupervised Image-to-Image Translation Using Domain-Specific Variational Information Bound},
  booktitle = NeurIPS,
  year      = {2018},
  pdf       = {https://proceedings.neurips.cc/paper/2018/file/c7c46d4baf816bfb07c7f3bf96d88544-Paper.pdf},
  abstract = {Unsupervised image-to-image translation is a class of computer vision problems which aims at modeling conditional distribution of images in the target domain, given a set of unpaired images in the source and target domains. An image in the source domain might have multiple representations in the target domain. Therefore, ambiguity in modeling of the conditional distribution arises, specially when the images in the source and target domains come from different modalities. Current approaches mostly rely on simplifying assumptions to map both domains into a shared-latent space. Consequently, they are only able to model the domain-invariant information between the two modalities. These approaches usually fail to model domain-specific information which has no representation in the target domain. In this work, we propose an unsupervised image-to-image translation framework which maximizes a domain-specific variational information bound and learns the target domain-invariant representation of the two domain. The proposed framework makes it possible to map a single source image into multiple images in the target domain, utilizing several target domain-specific codes sampled randomly from the prior distribution, or extracted from reference images.},
        selected={true}
}


@inproceedings{2,
 abbr={WACV},
  mainlink = {https://arxiv.org/pdf/1809.08999.pdf},
         img = {assets/img/geo1.jpg},
  author    = {Ali Dabouei and
               Sobhan Soleymani and
               Jeremy Dawson and
               Nasser M. Nasrabadi},
  title     = {Fast Geometrically-Perturbed Adversarial Faces},
  booktitle = WACV,
  year      = {2019},
  arxiv       = {1809.08999},
  code     =    {https://github.com/sbnsl/FLM},
  abstract = {The state-of-the-art performance of deep learning algorithms has led to a considerable increase in the utilization of machine learning in security-sensitive and critical applications. However, it has recently been shown that a small and carefully crafted perturbation in the input space can completely fool a deep model. In this study, we explore the extent to which face recognition systems are vulnerable to geometrically-perturbed adversarial faces. We propose a fast landmark manipulation method for generating adversarial faces, which is approximately 200 times faster than the previous geometric attacks and obtains 99.86% success rate on the state-of-the-art face recognition models. To further force the generated samples to be natural, we introduce a second attack constrained on the semantic structure of the face which has the half speed of the first attack with the success rate of 99.96%. Both attacks are extremely robust against the state-of-the-art defense methods with the success rate of equal or greater than 53.59%.},
        selected={true}
}


@inproceedings{1,
 abbr={ICPR},
  mainlink = {https://ieeexplore.ieee.org/abstract/document/8545061},
         img = {assets/img/ICPR2018.png},
  author    = {Sobhan Soleymani and Ali Dabouei and Hadi Kazemi
               and Jeremy Dawson and
               Nasser M. Nasrabadi},
  title     = {Multi-Level Feature Abstraction from Convolutional Neural Networks for Multimodal Biometric Identification},
  booktitle = ICPR,
  year      = {2018},
  arxiv       = {1807.01332},
  abstract = {In this paper, we propose a deep multimodal fusion network to fuse multiple modalities (face, iris, and fingerprint) for person identification. The proposed deep multimodal fusion algorithm consists of multiple streams of modality-specific Convolutional Neural Networks (CNNs), which are jointly optimized at multiple feature abstraction levels. Multiple features are extracted at several different convolutional layers from each modality-specific CNN for joint feature fusion, optimization,and classification. Features extracted at different convolutional layers of a modality-specific CNN represent the input at several different levels of abstract representations. We demonstrate that an efficient multimodal classification can be accomplished with a significant reduction in the number of network parameters by exploiting these multi-level abstract representations extracted from all the modality-specific CNNs. We demonstrate an increase in multimodal person identification performance by utilizing the proposed multi-level feature abstract representations in our multimodal fusion, rather than using only the features from the last layer of each modality-specific CNNs. We show that our deep multi-modal CNNs with multimodal fusion at several different feature level abstraction can significantly outperform the unimodal representation accuracy. We also demonstrate that the joint optimization of all the modality-specific CNNs excels the score and decision level fusions of independently optimized CNNs.},
        selected={true}
}
