---
---

@string{TBIOM = {TBIOM}}
@string{CVPR = {CVPR}}
@string{ICLR = {ICLR}}
@string{ICCV = {ICCV}}
@string{WACV = {WACV}}
@string{= {BMVC}}
@string{ECCV = {ECCV}}
@string{NeurIPS = {NeurIPS}}


@inproceedings{12,
 abbr={TBIOM},
 mainlink = {https://ieeexplore.ieee.org/abstract/document/9631949},
  img = {assets/img/supermix.jpg},
  author    = {Sobhan Soleymani and Ali Dabouei and Fariborz Taherkhani Seyed Mehdi Iranmanesh and Jeremy Dawson and Nasser M. Nasrabadi},
  title     = {Quality-Aware Multimodal Biometric Recognition},
  booktitle = TBIOM,
  year      = {2021},
  pdf       = {https://arxiv.org/pdf/2112.05827.pdf},
  abstract = {We present a quality-aware multimodal recognition framework that combines representations from multiple biometric traits with varying quality and number of samples to achieve increased recognition accuracy by extracting complimentary identification information based on the quality of the samples. We develop a quality-aware framework for fusing representations of input modalities by weighting their importance using quality scores estimated in a weakly-supervised fashion. This framework utilizes two fusion blocks, each represented by a set of quality-aware and aggregation networks. In addition to architecture modifications, we propose two task-specific loss functions: multimodal separability loss and multimodal compactness loss. The first loss assures that the representations of modalities for a class have comparable magnitudes to provide a better quality estimation, while the multimodal representations of different classes are distributed to achieve maximum discrimination in the embedding space. The second loss, which is considered to regularize the network weights, improves the generalization performance by regularizing the framework. We evaluate the performance by considering three multimodal datasets consisting of face, iris, and fingerprint modalities. The efficacy of the framework is demonstrated through comparison with the state-of-the-art algorithms. In particular, our framework outperforms the rank- and score-level fusion of modalities of BIOMDATA by more than 30% for true acceptance rate at false acceptance rate of 10^{-4}.},
        selected={true}
}


@inproceedings{11,
 abbr={CVPR},
 mainlink = {https://openaccess.thecvf.com/content/CVPR2021/html/Dabouei_SuperMix_Supervising_the_Mixing_Data_Augmentation_CVPR_2021_paper.html},
  img = {assets/img/supermix.jpg},
  author    = {Ali Dabouei and Sobhan Soleymani and Fariborz Taherkhani and Nasser M. Nasrabadi},
  title     = {SuperMix: Supervising the Mixing Data Augmentation},
  booktitle = CVPR,
  year      = {2021},
  pdf       = {https://openaccess.thecvf.com/content/CVPR2021/papers/Dabouei_SuperMix_Supervising_the_Mixing_Data_Augmentation_CVPR_2021_paper.pdf},
  code      = {https://github.com/alldbi/SuperMix},
  abstract = {This paper presents a supervised mixing augmentation method termed SuperMix, which exploits the salient regions within input images to construct mixed training samples. SuperMix is designed to obtain mixed images rich in visual features and complying with realistic image priors. To enhance the efficiency of the algorithm, we develop a variant of the Newton iterative method, 65xfaster than gradient descent on this problem. We validate the effectiveness of SuperMix through extensive evaluations and ablation studies on two tasks of object classification and knowledge distillation. On the classification task, SuperMix provides comparable performance to the advanced augmentation methods, such as AutoAugment and RandAugment. In particular, combining SuperMix with RandAugment achieves 78.2% top-1 accuracy on ImageNet with ResNet50. On the distillation task, solely classifying images mixed using the teacher's knowledge achieves comparable performance to the state-of-the-art distillation methods. Furthermore, on average, incorporating mixed images into the distillation objective improves the performance by 3.4% and 3.1% on CIFAR-100 and ImageNet, respectively.},
        selected={true}
}

@inproceedings{10,
 abbr={CVPR},
  mainlink = {https://openaccess.thecvf.com/content/CVPR2021/html/Taherkhani_Self-Supervised_Wasserstein_Pseudo-Labeling_for_Semi-Supervised_Image_Classification_CVPR_2021_paper.html},
 img = {assets/img/far_cvpr21.jpg},
  author    = {Fariborz Taherkhani and Ali Dabouei and Sobhan Soleymani and Jeremy Dawson and Nasser M. Nasrabadi},
  title     = {Self-Supervised Wasserstein Pseudo-Labeling for Semi-Supervised Image Classification},
  booktitle = CVPR,
  year      = {2021},
  pdf       = {https://openaccess.thecvf.com/content/CVPR2021/papers/Taherkhani_Self-Supervised_Wasserstein_Pseudo-Labeling_for_Semi-Supervised_Image_Classification_CVPR_2021_paper.pdf},
  abstract = {The goal is to use Wasserstein metric to provide pseudo labels for the unlabeled images to train a Convolutional Neural Networks (CNN) in a Semi-Supervised Learning (SSL) manner for the classification task. The basic premise in our method is that the discrepancy between two discrete empirical measures (e.g., clusters) which come from the same or similar distribution is expected to be less than the case where these measures come from completely two different distributions. In our proposed method, we first pre-train our CNN using a self-supervised learning method to make a cluster assumption on the unlabeled images. Next, inspired by the Wasserstein metric which considers the geometry of the metric space to provide a natural notion of similarity between discrete empirical measures, we leverage it to cluster the unlabeled images and then match the clusters to their similar class of labeled images to provide a pseudo label for the data within each cluster. We have evaluated and compared our method with state-of-the-art SSL methods on the standard datasets to demonstrate its effectiveness.},
        selected={true}
}

@inproceedings{9,
 abbr={WACV},
  mainlink = {https://openaccess.thecvf.com/content/WACV2021/html/Soleymani_Mutual_Information_Maximization_on_Disentangled_Representations_for_Differential_Morph_Detection_WACV_2021_paper.html},
 img = {assets/img/sobhanwacv.jpg},
  author    = {Sobhan Soleymani and Ali Dabouei and Fariborz Taherkhani and Jeremy Dawson and Nasser M. Nasrabadi},
  title     = {Mutual Information Maximization on Disentangled Representations for Differential Morph Detection},
  booktitle = WACV,
  year      = {2021},
  pdf       = {https://www.ecva.net/papers/eccv_2020/papers_ECCV/papers/123490494.pdf},
  abstract = {In this paper, we present a novel differential morph detection framework, utilizing landmark and appearance disentanglement. In our framework, the face image is represented in the embedding domain using two disentangled but complementary representations. The network is trained by triplets of face images, in which the intermediate image inherits the landmarks from one image and the appearance from the other image. This initially trained network is further trained for each dataset using contrastive representations. We demonstrate that, by employing appearance and landmark disentanglement, the proposed framework can provide state-of-the-art differential morph detection performance. This functionality is achieved by the using distances in landmark, appearance, and ID domains. The performance of the proposed framework is evaluated using three morph datasets generated with different methodologies.},
        selected={true}
}

@inproceedings{8,
 abbr={ECCV},
  mainlink = {https://www.ecva.net/papers/eccv_2020/papers_ECCV/html/2402_ECCV_2020_paper.php},
  img = {assets/img/fariborzeccv.jpg},
  author    = {Fariborz Taherkhani and Ali Dabouei and Sobhan Soleymani and Jeremy Dawson and Nasser M. Nasrabadi},
  title     = {Transporting Labels via Hierarchical Optimal Transport for Semi-Supervised Learning},
  booktitle = ECCV,
  year      = {2020},
  pdf       = {https://www.ecva.net/papers/eccv_2020/papers_ECCV/papers/123490494.pdf},
  abstract = {Semi-Supervised Learning (SSL) based on Convolutional Neural Networks (CNNs) have recently been proven as powerful tools for standard tasks such as image classification when there is not a sufficient amount of labeled data available during the training. In this work, we consider the general setting of the SSL problem for image classification, where the labeled and unlabeled data come from the same underlying distribution. We propose a new SSL method that adopts a hierarchical Optimal Transport (OT) technique to find a mapping from empirical unlabeled measures to corresponding labeled measures by leveraging the minimum amount of transportation cost in the label space. Based on this mapping, pseudo-labels for the unlabeled data are inferred, which are then used along with the labeled data for training the CNN. We evaluated and compared our method with state-of-the-art SSL approaches on standard datasets to demonstrate the superiority of our SSL method.},
        selected={true}
}

@inproceedings{7,
 abbr={BMVC},
  mainlink = {https://www.bmvc2020-conference.com/assets/papers/0561.pdf},
   img = {assets/img/mehdibmvc.jpg},
  author    = {Mehdi Iranmanesh and Ali Dabouei and Nasser M. Nasrabadi},
  title     = {Attribute Adaptive Margin Softmax Loss using Privileged Information},
  booktitle = BMVC,
  year      = {2020},
  pdf       = {https://www.bmvc2020-conference.com/assets/papers/0561.pdf},
  abstract = {We present a novel framework to exploit privileged information for recognition which is provided only during the training phase. Here, we focus on recognition task where images are provided as the main view and soft biometric traits (attributes) are provided as the privileged data (only available during training phase). We demonstrate that more discriminative feature space can be learned by enforcing a deep network to adjust adaptive margins between classes utilizing attributes. This tight constraint also effectively reduces the class imbalance inherent in the local data neighborhood, thus carving more balanced class boundaries locally and using feature space more efficiently. Extensive experiments are performed on five different datasets and the results show the superiority of our method compared to the state-of-the-art models in both tasks of face recognition and person re-identification.},
        selected={true}
}

@inproceedings{6,
 abbr={CVPR},
  mainlink = {https://openaccess.thecvf.com/content_CVPR_2020/html/Dabouei_Exploiting_Joint_Robustness_to_Adversarial_Perturbations_CVPR_2020_paper.html},
    img = {assets/img/gpmr.jpg},
  author    = {Ali Dabouei and Sobhan Soleymani and Fariborz Taherkhani and Jeremy Dawson and Nasser M. Nasrabadi},
  title     = {Exploiting Joint Robustness to Adversarial Perturbations},
  booktitle = CVPR,
  year      = {2020},
  pdf       = {https://openaccess.thecvf.com/content_CVPR_2020/papers/Dabouei_Exploiting_Joint_Robustness_to_Adversarial_Perturbations_CVPR_2020_paper.pdf},
  abstract = {Recently, ensemble models have demonstrated empirical capabilities to alleviate the adversarial vulnerability. In this paper, we exploit first-order interactions within ensembles to formalize a reliable and practical defense. We introduce a scenario of interactions that certifiably improves the robustness according to the size of the ensemble, the diversity of the gradient directions, and the balance of the member's contribution to the robustness. We present a joint gradient phase and magnitude regularization (GPMR) as a vigorous approach to impose the desired scenario of interactions among members of the ensemble. Through extensive experiments, including gradient-based and gradient-free evaluations on several datasets and network architectures, we validate the practical effectiveness of the proposed approach compared to the previous methods. Furthermore, we demonstrate that GPMR is orthogonal to other defense strategies developed for single classifiers and their combination can further improve the robustness of ensembles.},
        selected={true}
}

@inproceedings{5,
 abbr={WACV},
  mainlink = {https://openaccess.thecvf.com/content_WACV_2020/html/Dabouei_SmoothFool_An_Efficient_Framework_for_Computing_Smooth_Adversarial_Perturbations_WACV_2020_paper.html},
     img = {assets/img/smoothfool.jpg},
  author    = {Ali Dabouei and Sobhan Soleymani and Fariborz Taherkhani and Jeremy Dawson and Nasser M. Nasrabadi},
  title     = {SmoothFool: An Efficient Framework for Computing Smooth Adversarial Perturbations},
  booktitle = WACV,
  year      = {2020},
  pdf       = {https://openaccess.thecvf.com/content_WACV_2020/papers/Dabouei_SmoothFool_An_Efficient_Framework_for_Computing_Smooth_Adversarial_Perturbations_WACV_2020_paper.pdf},
  code      = {https://github.com/alldbi/SmoothFool},
  abstract = {Deep neural networks are susceptible to adversarial manipulations in the input domain. The extent of vulnerability has been explored intensively in cases of l_p-bounded and l_p-minimal adversarial perturbations. However, the vulnerability of DNNs to adversarial perturbations with specific statistical properties or frequency-domain characteristics has not been sufficiently explored. In this paper, we study the smoothness of perturbations and propose SmoothFool, a general and computationally efficient framework for computing smooth adversarial perturbations. Through extensive experiments, we validate the efficacy of the proposed method for both the white-box and black-box attack scenarios. In particular, we demonstrate that: (i) there exist extremely smooth adversarial perturbations for well-established and widely used network architectures, (ii) smoothness significantly enhances the robustness of perturbations against state-of-the-art defense mechanisms, (iii) smoothness improves the transferability of adversarial perturbations across both data points and network architectures, and (iv) class categories exhibit a variable range of susceptibility to smooth perturbations. Our results suggest that smooth APs can play a significant role in exploring the vulnerability extent of DNNs to adversarial examples..},
        selected={true}
}

@inproceedings{4,
 abbr={WACV},
  mainlink = {https://openaccess.thecvf.com/content_WACV_2020/html/Dabouei_Boosting_Deep_Face_Recognition_via_Disentangling_Appearance_and_Geometry_WACV_2020_paper.html},
      img = {assets/img/boost.jpg},
  author    = {Ali Dabouei and Fariborz Taherkhani and Sobhan Soleymani and Jeremy Dawson and Nasser M. Nasrabadi},
  title     = {Boosting Deep Face Recognition via Disentangling Appearance and Geometry},
  booktitle = WACV,
  year      = {2020},
  pdf       = {https://openaccess.thecvf.com/content_WACV_2020/papers/Dabouei_Boosting_Deep_Face_Recognition_via_Disentangling_Appearance_and_Geometry_WACV_2020_paper.pdf},
  abstract = {In this paper, we propose a framework for disentangling the appearance and geometry representations in the face recognition task. To provide supervision for this aim, we generate geometrically identical faces by incorporating spatial transformations. We demonstrate that the proposed approach enhances the performance of deep face recognition models by assisting the training process in two ways. First, it enforces the early and intermediate convolutional layers to learn more representative features that satisfy the properties of disentangled embeddings. Second, it augments the training set by altering faces geometrically. Through extensive experiments, we demonstrate that integrating the proposed approach into state-of-the-art face recognition methods effectively improves their performance on challenging datasets, such as LFW, YTF, and MegaFace. Both theoretical and practical aspects of the method are analyzed rigorously by concerning ablation studies and knowledge transfer tasks. Furthermore, we show that the knowledge leaned by the proposed method can favor other face-related tasks, such as attribute prediction.},
        selected={true}
}

@inproceedings{3,
 abbr={WACV},
  mainlink = {https://openaccess.thecvf.com/content_WACV_2020/html/Iranmanesh_Robust_Facial_Landmark_Detection_via_Aggregation_on_Geometrically_Manipulated_Faces_WACV_2020_paper.html},
       img = {assets/img/mehdi1.jpg},
  author    = {Seyed Mehdi Iranmanesh and Ali Dabouei and Sobhan Soleymani and Hadi Kazemi and Nasser Nasrabadi},
  title     = {Robust Facial Landmark Detection via Aggregation on Geometrically Manipulated Faces},
  booktitle = WACV,
  year      = {2020},
  pdf       = {https://openaccess.thecvf.com/content_ICCV_2019/papers/Taherkhani_A_Weakly_Supervised_Fine_Label_Classifier_Enhanced_by_Coarse_Supervision_ICCV_2019_paper.pdf},
  abstract = {In this work, we present a practical approach to the problem of facial landmark detection. The proposed method can deal with large shape and appearance variations under the rich shape deformation. To handle the shape variations we equip our method with the aggregation of manipulated face images. The proposed framework generates different manipulated faces using only one given face image. The approach utilizes the fact that small but carefully crafted geometric manipulation in the input domain can fool deep face recognition models. We propose three different approaches to generate manipulated faces in which two of them perform the manipulations via adversarial attacks and the other one uses known transformations. Aggregating the manipulated faces provides a more robust landmark detection approach which is able to capture more important deformations and variations of the face shapes. Our approach is demonstrated its superiority compared to the state-of-the-art method on benchmark datasets AFLW, 300-W, and COFW.},
        selected={true}
}

@inproceedings{2,
 abbr={ICCV},
  mainlink = {https://openaccess.thecvf.com/content_ICCV_2019/html/Taherkhani_A_Weakly_Supervised_Fine_Label_Classifier_Enhanced_by_Coarse_Supervision_ICCV_2019_paper.html},
        img = {assets/img/f1.jpg},
  author    = {Fariborz Taherkhani and
                Hadi Kazemi and Ali Dabouei and Jeremy Dawson and Nasser M. Nasrabadi},
  title     = {A Weakly Supervised Fine Label Classifier Enhanced by Coarse Supervision},
  booktitle = ICCV,
  year      = {2019},
  pdf       = {https://openaccess.thecvf.com/content_ICCV_2019/papers/Taherkhani_A_Weakly_Supervised_Fine_Label_Classifier_Enhanced_by_Coarse_Supervision_ICCV_2019_paper.pdf},
  abstract = {Objects are usually organized in a hierarchical structure in which each coarse category (e.g., big cat) corresponds to a superclass of several fine categories (e.g., cheetah, leopard). The objects grouped within the same coarse category, but in different fine categories, usually share a set of global visual features; however, these objects have distinctive local properties that characterize them at a fine level. This paper addresses the challenge of fine image classification in a weakly supervised fashion, whereby a subset of images is tagged by fine labels, while the remaining are tagged by coarse labels. We propose a new deep model that leverages coarse images to improve the classification performance of fine images within the coarse category. Our model is an end to end framework consisting of a Convolutional Neural Network (CNN) which uses both fine and coarse images to tune its parameters. The CNN outputs are then fanned out into two separate branches such that the first branch uses a supervised low rank self expressive layer to project the CNN outputs to the low rank subspaces to capture the global structures for the coarse classification, while the other branch uses a supervised sparse self expressive layer to project them to the sparse subspaces to capture the local structures for the fine classification. Our deep model uses coarse images in conjunction with fine images to jointly explore the low rank and sparse subspaces by sharing the parameters during the training which causes the data points obtained by the CNN to be well-projected to both sparse and low rank subspaces for classification.},
        selected={true}
}


@inproceedings{1,
 abbr={WACV},
  mainlink = {https://arxiv.org/pdf/1809.08999.pdf},
         img = {assets/img/geo1.jpg},
  author    = {Ali Dabouei and
               Sobhan Soleymani and
               Jeremy Dawson and
               Nasser M. Nasrabadi},
  title     = {Fast Geometrically-Perturbed Adversarial Faces},
  booktitle = WACV,
  year      = {2019},
  arxiv       = {1809.08999},
  code     =    {https://github.com/alldbi/FLM},
  abstract = {The state-of-the-art performance of deep learning algorithms has led to a considerable increase in the utilization of machine learning in security-sensitive and critical applications. However, it has recently been shown that a small and carefully crafted perturbation in the input space can completely fool a deep model. In this study, we explore the extent to which face recognition systems are vulnerable to geometrically-perturbed adversarial faces. We propose a fast landmark manipulation method for generating adversarial faces, which is approximately 200 times faster than the previous geometric attacks and obtains 99.86% success rate on the state-of-the-art face recognition models. To further force the generated samples to be natural, we introduce a second attack constrained on the semantic structure of the face which has the half speed of the first attack with the success rate of 99.96%. Both attacks are extremely robust against the state-of-the-art defense methods with the success rate of equal or greater than 53.59%.},
        selected={true}
}
